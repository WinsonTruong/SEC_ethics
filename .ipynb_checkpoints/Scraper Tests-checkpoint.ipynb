{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the SEC Litigations Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project represents all four stages of the data science lifecycle:\n",
    "\n",
    "(1) gathering data\n",
    "(2) asking questions\n",
    "(3) conducting analysis\n",
    "(4) drawing conclusions\n",
    "\n",
    "It aims to better organize and understand SEC litigations using a variety of packages and tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing helpful tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.cElementTree as et\n",
    "\n",
    "from IPython.display import display, Latex, Markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSS Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first start with scraping the RSS feed which is a live, neatly formatted update of litigations of the SEC. Here is the link I'll be using: https://www.sec.gov/rss/litigation/litreleases.xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedXML = et.parse(\"litigationData/sec11.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in parsedXML.getroot():\n",
    "    item = node.attrib.get('item')\n",
    "    title = node.find('title')\n",
    "    link = node.find('link')\n",
    "    description = node.find('description')\n",
    "    guid = node.find('guid')\n",
    "    pubDate = node.find('pubDate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvalueofnode(node):\n",
    "    \"\"\" return node text or None \"\"\"\n",
    "    return node.text if node is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Had to delete the <CHANNEL> tag from the file but at least it works \n",
    "\n",
    "parsed_xml = et.parse(\"litigationRSS/RSS_oct_4.xml\")\n",
    "dfcols = ['title', 'link', 'description', 'guid', 'pubDate']\n",
    "df_xml = pd.DataFrame(columns=dfcols)\n",
    " \n",
    "for node in parsed_xml.getroot():\n",
    "    item = node.attrib.get('item')\n",
    "    title = node.find('title')\n",
    "    link = node.find('link')\n",
    "    description = node.find('description')\n",
    "    guid = node.find('guid')\n",
    "    pubDate = node.find('pubDate')\n",
    "\n",
    "    df_xml = df_xml.append(\n",
    "        pd.Series([getvalueofnode(title), getvalueofnode(link),\n",
    "                   getvalueofnode(description), getvalueofnode(guid),\n",
    "                   getvalueofnode(pubDate)], index=dfcols), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>description</th>\n",
       "      <th>guid</th>\n",
       "      <th>pubDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Christopher J. Spencer and John Busshaus</td>\n",
       "      <td>https://www.sec.gov/litigation/litreleases/201...</td>\n",
       "      <td>SEC Charges CEO and CFO of Digital Entertainme...</td>\n",
       "      <td>LR-24636</td>\n",
       "      <td>Fri, 04 Oct 2019 11:20:02 EDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PlexCorps, Dominic Lacroix, and Sabrina Paradi...</td>\n",
       "      <td>https://www.sec.gov/litigation/litreleases/201...</td>\n",
       "      <td>Defendants Charged in Fraudulent ICO to Pay Ne...</td>\n",
       "      <td>LR-24635</td>\n",
       "      <td>Wed, 02 Oct 2019 16:54:27 EDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Woojae (\"Steve\") Jung, et al.</td>\n",
       "      <td>https://www.sec.gov/litigation/litreleases/201...</td>\n",
       "      <td>SEC Obtains Final Judgment Against Investment ...</td>\n",
       "      <td>LR-24634</td>\n",
       "      <td>Wed, 02 Oct 2019 08:54:06 EDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Westport Capital Markets, LLC and Christopher ...</td>\n",
       "      <td>https://www.sec.gov/litigation/litreleases/201...</td>\n",
       "      <td>SEC Obtains Partial Summary Judgment Against I...</td>\n",
       "      <td>LR-24633</td>\n",
       "      <td>Wed, 02 Oct 2019 08:18:02 EDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bluepoint Investment Counsel, et al.</td>\n",
       "      <td>https://www.sec.gov/litigation/litreleases/201...</td>\n",
       "      <td>SEC Announces Fraud Charges Related to Wiscons...</td>\n",
       "      <td>LR-24632</td>\n",
       "      <td>Mon, 30 Sep 2019 17:17:42 EDT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0           Christopher J. Spencer and John Busshaus   \n",
       "1  PlexCorps, Dominic Lacroix, and Sabrina Paradi...   \n",
       "2                      Woojae (\"Steve\") Jung, et al.   \n",
       "3  Westport Capital Markets, LLC and Christopher ...   \n",
       "4               Bluepoint Investment Counsel, et al.   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.sec.gov/litigation/litreleases/201...   \n",
       "1  https://www.sec.gov/litigation/litreleases/201...   \n",
       "2  https://www.sec.gov/litigation/litreleases/201...   \n",
       "3  https://www.sec.gov/litigation/litreleases/201...   \n",
       "4  https://www.sec.gov/litigation/litreleases/201...   \n",
       "\n",
       "                                         description      guid  \\\n",
       "0  SEC Charges CEO and CFO of Digital Entertainme...  LR-24636   \n",
       "1  Defendants Charged in Fraudulent ICO to Pay Ne...  LR-24635   \n",
       "2  SEC Obtains Final Judgment Against Investment ...  LR-24634   \n",
       "3  SEC Obtains Partial Summary Judgment Against I...  LR-24633   \n",
       "4  SEC Announces Fraud Charges Related to Wiscons...  LR-24632   \n",
       "\n",
       "                         pubDate  \n",
       "0  Fri, 04 Oct 2019 11:20:02 EDT  \n",
       "1  Wed, 02 Oct 2019 16:54:27 EDT  \n",
       "2  Wed, 02 Oct 2019 08:54:06 EDT  \n",
       "3  Wed, 02 Oct 2019 08:18:02 EDT  \n",
       "4  Mon, 30 Sep 2019 17:17:42 EDT  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xml.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovered that the RSS feed is just a subscription feed and does not include everything - still helpful to know and have the code to parse it in case. Now let's try scraping the HTML table on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1:  Sucesfully scraped; but not ideal format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_soup(url):\n",
    "    the_page = requests.get(url)\n",
    "    soup_data = BeautifulSoup(the_page.content, \"html.parser\")\n",
    "    return soup_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with 2010:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2010 = make_soup('https://www.sec.gov/litigation/litreleases/litrelarchive/litarchive2010.shtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "table = soup2010.find_all('table')[4] # target the specific table\n",
    "header, *rows = table.find_all('tr')\n",
    "\n",
    "for row in rows:\n",
    "    try:\n",
    "        litigation, date, complaint = row.find_all('td')\n",
    "    except ValueError:\n",
    "        continue # ignore header row and quarter rows\n",
    "\n",
    "    id = litigation.text.strip().split('-')[-1]\n",
    "    date = date.text.strip()\n",
    "    desc = complaint.text.strip().split('\\t')[0]\n",
    "    lit_url = litigation.find('a').get('href')\n",
    "\n",
    "    try:\n",
    "        comp_url = complaint.find('a').get('href')\n",
    "    except AttributeError:\n",
    "        comp_ulr = None # complaint url is optional\n",
    "\n",
    "    info = dict(id=id, date=date, desc=desc, lit_url=lit_url, comp_url=comp_url)\n",
    "    data.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '21795',\n",
       "  'date': 'Dec 27, 2010',\n",
       "  'desc': 'Alcatel-Lucent, S.A.',\n",
       "  'lit_url': '/litigation/litreleases/2010/lr21795.htm',\n",
       "  'comp_url': '/litigation/complaints/2010/comp21795.pdf'},\n",
       " {'id': '21794',\n",
       "  'date': 'Dec 23, 2010',\n",
       "  'desc': 'One or More Unknown Purchasers of Options of InterMune, Inc.',\n",
       "  'lit_url': '/litigation/litreleases/2010/lr21794.htm',\n",
       "  'comp_url': '/litigation/complaints/2010/comp21794.pdf'},\n",
       " {'id': '21793',\n",
       "  'date': 'Dec 23, 2010',\n",
       "  'desc': 'Michael E. Kelly, et al.',\n",
       "  'lit_url': '/litigation/litreleases/2010/lr21793.htm',\n",
       "  'comp_url': '/litigation/complaints/2010/comp21794.pdf'},\n",
       " {'id': '21792',\n",
       "  'date': 'Dec 23, 2010',\n",
       "  'desc': 'One or More Unknown Purchasers of Securities of Martek Biosciences Corporation',\n",
       "  'lit_url': '/litigation/litreleases/2010/lr21792.htm',\n",
       "  'comp_url': '/litigation/complaints/2010/comp21794.pdf'},\n",
       " {'id': '21791',\n",
       "  'date': 'Dec 22, 2010',\n",
       "  'desc': 'Pharma Holdings, Inc., Edward Klapp IV and Edward Klapp Jr.',\n",
       "  'lit_url': '/litigation/litreleases/2010/lr21791.htm',\n",
       "  'comp_url': '/litigation/complaints/2010/comp21791.pdf'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2: Placing into a pandas dataframe; mostly functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEC 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import requests, re\n",
    "\n",
    "def make_soup(url):\n",
    "    the_page = requests.get(url)\n",
    "    soup_data = BeautifulSoup(the_page.content, \"html.parser\")\n",
    "    return soup_data\n",
    "\n",
    "soup2010 = make_soup('https://www.sec.gov/litigation/litreleases/litrelarchive/litarchive2010.shtml')\n",
    "releases = []\n",
    "links = []\n",
    "dates = []\n",
    "descs = [] \n",
    "addit_urls = []\n",
    "\n",
    "for i in soup2010.select('td:nth-of-type(1):has([href^=\"/litigation/litreleases/\"])'):\n",
    "    sib_sib = i.next_sibling.next_sibling.next_sibling.next_sibling\n",
    "    releases+= [i.a.text]\n",
    "    links+= [i.a['href']]\n",
    "    dates += [i.next_sibling.next_sibling.text.strip()]\n",
    "    descs += [re.sub('\\t+|\\s+',' ',sib_sib.text.strip())]\n",
    "    addit_urls += ['N/A' if sib_sib.a is None else sib_sib.a['href']]\n",
    "\n",
    "result = list(zip(releases, links, dates, descs, addit_urls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllink = []\n",
    "for i in links:\n",
    "    fulllink += ['https://www.sec.gov'+i]\n",
    "    \n",
    "additional_url = []\n",
    "for i in addit_urls:\n",
    "    if i == 'N/A':\n",
    "        additional_url += ['N/A']\n",
    "    else: \n",
    "        additional_url += ['https://www.sec.gov' + i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec2010 = pd.DataFrame({'title' : releases,\n",
    "                   'links' : fulllink,\n",
    "                   'dates' : dates,\n",
    "                   'descs' : descs,\n",
    "                   'additional url' : additional_url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>links</th>\n",
       "      <th>dates</th>\n",
       "      <th>descs</th>\n",
       "      <th>additional url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR-21795</td>\n",
       "      <td>https://www.sec.gov/litigation/litreleases/201...</td>\n",
       "      <td>Dec 27, 2010</td>\n",
       "      <td>Alcatel-Lucent, S.A.  See also: SEC Complaint</td>\n",
       "      <td>https://www.sec.gov/litigation/complaints/2010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR-21794</td>\n",
       "      <td>https://www.sec.gov/litigation/litreleases/201...</td>\n",
       "      <td>Dec 23, 2010</td>\n",
       "      <td>One or More Unknown Purchasers of Options of I...</td>\n",
       "      <td>https://www.sec.gov/litigation/complaints/2010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR-21793</td>\n",
       "      <td>https://www.sec.gov/litigation/litreleases/201...</td>\n",
       "      <td>Dec 23, 2010</td>\n",
       "      <td>Michael E. Kelly, et al.</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LR-21792</td>\n",
       "      <td>https://www.sec.gov/litigation/litreleases/201...</td>\n",
       "      <td>Dec 23, 2010</td>\n",
       "      <td>One or More Unknown Purchasers of Securities o...</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR-21791</td>\n",
       "      <td>https://www.sec.gov/litigation/litreleases/201...</td>\n",
       "      <td>Dec 22, 2010</td>\n",
       "      <td>Pharma Holdings, Inc., Edward Klapp IV and Edw...</td>\n",
       "      <td>https://www.sec.gov/litigation/complaints/2010...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      title                                              links         dates  \\\n",
       "0  LR-21795  https://www.sec.gov/litigation/litreleases/201...  Dec 27, 2010   \n",
       "1  LR-21794  https://www.sec.gov/litigation/litreleases/201...  Dec 23, 2010   \n",
       "2  LR-21793  https://www.sec.gov/litigation/litreleases/201...  Dec 23, 2010   \n",
       "3  LR-21792  https://www.sec.gov/litigation/litreleases/201...  Dec 23, 2010   \n",
       "4  LR-21791  https://www.sec.gov/litigation/litreleases/201...  Dec 22, 2010   \n",
       "\n",
       "                                               descs  \\\n",
       "0      Alcatel-Lucent, S.A.  See also: SEC Complaint   \n",
       "1  One or More Unknown Purchasers of Options of I...   \n",
       "2                           Michael E. Kelly, et al.   \n",
       "3  One or More Unknown Purchasers of Securities o...   \n",
       "4  Pharma Holdings, Inc., Edward Klapp IV and Edw...   \n",
       "\n",
       "                                      additional url  \n",
       "0  https://www.sec.gov/litigation/complaints/2010...  \n",
       "1  https://www.sec.gov/litigation/complaints/2010...  \n",
       "2                                                N/A  \n",
       "3                                                N/A  \n",
       "4  https://www.sec.gov/litigation/complaints/2010...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec2010.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great - it works for one year lets do it for all years!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import requests, re\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Initializing all the lists \"\"\"\n",
    "releases = []\n",
    "incomplete_links = []\n",
    "dates = []\n",
    "descs = [] \n",
    "addit_urls = []\n",
    "\n",
    "\n",
    "\"\"\"This is a helper function that turns any url to a BSoup object.\"\"\"\n",
    "def make_soup(url):\n",
    "    the_page = requests.get(url)\n",
    "    soup_data = BeautifulSoup(the_page.content, \"html.parser\")\n",
    "    return soup_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" Making a list of soup objects\"\"\"\n",
    "souplist = []\n",
    "years = ['2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']\n",
    "\n",
    "start = 'https://www.sec.gov/litigation/litreleases/litrelarchive/litarchive'\n",
    "end = '.shtml'\n",
    "\n",
    "for i in years:\n",
    "    souplist += [make_soup(start + i + end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Gathering the columns\"\"\"\n",
    "\n",
    "for k in souplist:\n",
    "    for i in k.select('td:nth-of-type(1):has([href^=\"/litigation/litreleases/\"])'):\n",
    "        sib_sib = i.next_sibling.next_sibling.next_sibling.next_sibling\n",
    "        releases+= [i.a.text]\n",
    "        incomplete_links+= [i.a['href']]\n",
    "        dates += [i.next_sibling.next_sibling.text.strip()]\n",
    "        descs += [re.sub('\\t+|\\s+',' ',sib_sib.text.strip())]\n",
    "        addit_urls += ['N/A' if sib_sib.a is None else sib_sib.a['href']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Adding link headers to make them more functional\"\"\"\n",
    "fulllink = []\n",
    "for i in incomplete_links:\n",
    "\tfulllink += ['https://www.sec.gov'+i]\n",
    "\n",
    "additional_url = []\n",
    "for i in addit_urls:\n",
    "    if i == 'N/A':\n",
    "        additional_url += ['N/A']\n",
    "    else: \n",
    "        additional_url += ['https://www.sec.gov' + i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compiling the dateaframe\"\"\"\n",
    "lit = pd.DataFrame({'title' : releases,\n",
    "                   'links' : fulllink,\n",
    "                   'dates' : dates,\n",
    "                   'descs' : descs,\n",
    "                   'additional url' : additional_url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>links</th>\n",
       "      <th>dates</th>\n",
       "      <th>descs</th>\n",
       "      <th>additional url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3012</td>\n",
       "      <td>3012</td>\n",
       "      <td>3012</td>\n",
       "      <td>3012</td>\n",
       "      <td>3012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3012</td>\n",
       "      <td>3011</td>\n",
       "      <td>1556</td>\n",
       "      <td>2634</td>\n",
       "      <td>1390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>LR-23062</td>\n",
       "      <td>https://www.sec.gov/litigation/litreleases/201...</td>\n",
       "      <td>Sep. 28, 2018</td>\n",
       "      <td>Galleon Management, LP, et al.</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title                                              links  \\\n",
       "count       3012                                               3012   \n",
       "unique      3012                                               3011   \n",
       "top     LR-23062  https://www.sec.gov/litigation/litreleases/201...   \n",
       "freq           1                                                  2   \n",
       "\n",
       "                dates                           descs additional url  \n",
       "count            3012                            3012           3012  \n",
       "unique           1556                            2634           1390  \n",
       "top     Sep. 28, 2018  Galleon Management, LP, et al.            N/A  \n",
       "freq               11                              10           1617  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3010"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Dropping the 2 duplicate values (ok because we have 3000 entries)\"\"\"\n",
    "\n",
    "\n",
    "lit = lit[lit['links'] != 'https://www.sec.gov/litigation/litreleases/2013/lr22633a.htm']\n",
    "len(lit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Converting from DF to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas import ExcelWriter\n",
    "\n",
    "\n",
    "#writer = ExcelWriter('seclitigations.xlsx')\n",
    "#lit.to_excel(writer)\n",
    "#writer.save()\n",
    "\n",
    "#DF TO CSV: Only run when necessary to update\n",
    "#lit.to_csv('litigations.xlsx', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 of the Data Science Lifecycle (gathering data) is on it way! On to EDA, reasking questions, and regather data if necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Reading the pdf files themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Goals:\n",
    "    1. Analyze the text per lin\n",
    "        a. download all the links into pdf that can be parsed\n",
    "        b. use pypdf2 to analyze all the pdfs\n",
    "        c. _analysis_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkLists = lit['links'].value_counts().index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.sec.gov/litigation/complaints/2018/comp24378.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is page 0\n",
      "<re.Match object; span=(690, 696), match='market'>\n",
      "this is page 1\n",
      "<re.Match object; span=(348, 354), match='market'>\n",
      "this is page 2\n",
      "<re.Match object; span=(615, 621), match='market'>\n",
      "this is page 3\n",
      "None\n",
      "this is page 4\n",
      "<re.Match object; span=(315, 321), match='market'>\n",
      "this is page 5\n",
      "None\n",
      "this is page 6\n",
      "<re.Match object; span=(142, 148), match='market'>\n",
      "this is page 7\n",
      "<re.Match object; span=(301, 307), match='market'>\n",
      "this is page 8\n",
      "None\n",
      "this is page 9\n",
      "None\n",
      "this is page 10\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "# open the pdf file\n",
    "object = PyPDF2.PdfFileReader(\"testcomplaint.pdf\")\n",
    "\n",
    "# get number of pages\n",
    "NumPages = object.getNumPages()\n",
    "\n",
    "# define keyterms\n",
    "String = \"market\"\n",
    "\n",
    "# extract text and do the search\n",
    "for i in range(0, NumPages):\n",
    "    PageObj = object.getPage(i)\n",
    "    print(\"this is page \" + str(i)) \n",
    "    Text = PageObj.extractText() \n",
    "    # print(Text)\n",
    "    ResSearch = re.search(String, Text)\n",
    "    print(ResSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NumPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileMerger\n",
    "import pdftotext\n",
    "\n",
    "# Load your PDF\n",
    "with open(\"testcomplaint\", \"rb\") as f:\n",
    "    pdf = pdftotext.PDF(f)\n",
    "\n",
    "# Save all text to a txt file.\n",
    "with open('output.txt', 'w') as f:\n",
    "    f.write(\"\\n\\n\".join(pdf))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
